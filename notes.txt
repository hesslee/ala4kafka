ALA Error : 331818, Failed to accept link


drop table purchases;
CREATE TABLE purchases (
c1 INT PRIMARY KEY ,
c2 VARCHAR(20),
c3 int
);
drop table test_purchases;
CREATE TABLE test_purchases (
c1 INT PRIMARY KEY ,
c2 VARCHAR(20),
c3 int
);
CREATE REPLICATION ALA1 FOR ANALYSIS WITH '127.0.0.1', 47146 FROM sys.purchases TO sys.purchases;
ALTER REPLICATION ALA1 START;
drop REPLICATION ALA1;

drop sequence seq1;
create sequence seq1;
drop TABLE users;
CREATE TABLE users (
id INT default seq1.nextval PRIMARY KEY ,
name VARCHAR(20),
age int,
gender char(1)
);
insert into users (name,age,gender) values ('foo,bar', 20, 'M');
select * from users;


## create altibase JDBC source connector using query
$ curl -X DELETE "http://localhost:8083/connectors/altibase-source-connector"
$ curl --location --request POST 'http://localhost:8083/connectors' \
--header 'Content-Type: application/json' \
--data-raw '
{
    "name": "altibase-source-connector",
    "config": {
        "connector.class": "io.confluent.connect.jdbc.JdbcSourceConnector",
        "tasks.max": "1",
        "connection.url": "jdbc:Altibase://192.168.1.113:24447/mydb",
        "connection.user": "sys",
        "connection.password": "manager",
        "quote.sql.identifiers": "NEVER",
        "mode": "incrementing",
        "incrementing.column.name": "ID",
        "topic.prefix": "ALTIBASE_",
        "table.whitelist": "USER"
    }
}'

$ kafka-topics --list   --bootstrap-server localhost:9092
$ kafka-topics --delete --topic  purchases   --bootstrap-server localhost:9092
$ kafka-topics --create --topic  PURCHASES   --bootstrap-server localhost:9092
$ kafka-topics  --describe --topic purchases --bootstrap-server localhost:9092
$ kafka-console-consumer --topic PURCHASES --bootstrap-server localhost:9092 --from-beginning
$ kafka-console-consumer --topic ALTIBASE_USER --bootstrap-server localhost:9092 --from-beginning

> 이전 타임쉬트에 기술한 매 레코드마다, JSON 내부에 schema도 같이 저장하는 방식으로 아래와 같은 설정으로 정상동작함을 확인 하였습니다.

drop table test_purchases;
CREATE TABLE test_purchases (
c1 INT PRIMARY KEY ,
c2 VARCHAR(20),
c3 int
);

$ curl -X GET "http://localhost:8083/connectors/altibase-sink-connector/config"
$ curl -X DELETE "http://localhost:8083/connectors/altibase-sink-connector"
$ curl --location --request POST 'http://localhost:8083/connectors' \
--header 'Content-Type: application/json' \
--data-raw '
{
    "name": "altibase-sink-connector",
    "config": {
        "connector.class": "io.confluent.connect.jdbc.JdbcSinkConnector",
        "tasks.max": "1",
        "connection.url": "jdbc:Altibase://192.168.1.113:24447/mydb",
        "connection.user": "SYS",
        "connection.password": "MANAGER",
        "quote.sql.identifiers": "NEVER",
        "delete.enabled": true,
        "insert.mode": "upsert",
        "pk.mode": "record_key",
        "key.converter": "org.apache.kafka.connect.json.JsonConverter",
        "value.converter": "org.apache.kafka.connect.json.JsonConverter",
        "table.name.format": "TEST_${topic}",
        "topics": "PURCHASES"
    }
}'

$ curl -X DELETE "http://localhost:8083/connectors/altibase-sink-connector2"
$ curl --location --request POST 'http://localhost:8083/connectors' \
--header 'Content-Type: application/json' \
--data-raw '
{
    "name": "altibase-sink-connector2",
    "config": {
        "connector.class": "io.confluent.connect.jdbc.JdbcSinkConnector",
        "tasks.max": "1",
        "connection.url": "jdbc:Altibase://192.168.1.113:24447/mydb",
        "connection.user": "sys",
        "connection.password": "manager",
        "quote.sql.identifiers": "NEVER",
        "insert.mode": "upsert",
        "pk.mode": "record_key",
        "table.name.format": "TEST_${topic}",
        "topics": "ALTIBASE_USER"
    }
}'

$ curl -X GET http://localhost:8081/subjects
$ curl -X GET http://localhost:8081/subjects/ALTIBASE_USER-value/versions/1

> kafka connect 의 기본방식은 schema registry를 이용하고, avro 형식으로 topic에 저장하는 것임을 확인하였습니다.
> 일단은 schema registry에 접근하는 C API를 못찾았고, 또한, schema registry와 상호작용하는 방식도 나름의 복잡도 또는 성능 문제도 있을수 있을것 같습니다.
> 다른 방안은 매 레코드마다, JSON 내부에 schema도 같이 저장하는 방식입니다. 이것은 매 레코드의 크기가 커지는 문제가 있긴 합니다.
> 그렇지만, 성능 병목이 여기서 생기는 것은 아니고, 실제 DBMS에 apply 하는 곳에서 발생하므로, 
> 일단, 매 레코드마다, JSON 내부에 schema도 같이 저장하는 방식으로 진행해 봅니다.

> 아래와 같은 형태로 JSON string을 key와 value에 대하여 각각 만들어야 합니다.
> 예제 테이블: CREATE TABLE purchases (c1 INT PRIMARY KEY, c2 VARCHAR(20), c3 int);
> 예제 JSON:

#### key part
{
  "schema": {
    "type": "struct",
    "fields": [
      {
        "type": "int32",
        "optional": false,
        "field": "C1"
      }
    ],
    "optional": false,
    "name": "key"
  },
  "payload": {
    "C1": 42
  }
}


#### value part
{
  "schema": {
    "type": "struct",
    "fields": [
      {
        "type": "string",
        "optional": false,
        "field": "C2"
      },
      {
        "type": "int32",
        "optional": false,
        "field": "C3"
      }
    ],
    "optional": false,
    "name": "value"
  },
  "payload": {
    "C2": "Hello, world!",
    "C3": 123
  }
}

--------------------------------------------------
42          Hello, world!         123
1 row selected.

